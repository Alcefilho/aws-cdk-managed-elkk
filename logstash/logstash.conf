## inputs from Amazon Managed Kafka
input {
  kafka {
    bootstrap_servers => "$kafka_brokers"
    topics_pattern => ".*"
    codec => "json"
    decorate_events => true
    }
}
## filter to tidy up 
filter {
    # add kafka fields
    mutate {
        add_field => {"[kafka][topic]" => "%{[@metadata][kafka][topic]}"}
        add_field => {"[kafka][consumer_group]" => "%{[@metadata][kafka][consumer_group]}"}
        add_field => {"[kafka][partition]" => "%{[@metadata][kafka][partition]}"}
        add_field => {"[kafka][offset]" => "%{[@metadata][kafka][offset]}"}
        add_field => {"[kafka][key]" => "%{[@metadata][kafka][key]}"}
        add_field => {"[kafka][timstamp]" => "%{[@metadata][kafka][timestamp]}"}
    }
    # grok apache log
    grok {
        match => { "message" => "%{COMMONAPACHELOG}" }
    }
    # add log type field if grok succeeds and remove message 
    if "_grokparsefailure" not in [tags] {
        mutate { 
            add_field => { "[log][type]" => "COMMONAPACHELOG" }
            remove_field => [ "message" ]
        }
    }
}
## output to Amazon Elasticsearch Service
output {
    amazon_es {
        hosts => [ "$es_endpoint" ]
        region => "$elkk_region"
        index => "elkk-%{[kafka][topic]}-%{+YYYY.MM.dd}"
        codec => "json"
    }
## output to s3
    s3 {
        region => "$elkk_region"
        bucket => "$s3_bucket"
        size_file => 2048
        time_file => 5
        codec => "json"
        prefix => "elkk-%{[kafka][topic]}/%{+YYYY}/%{+MM}/%{+dd}"
    }
}